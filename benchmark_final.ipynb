{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T23:32:47.748583Z",
     "start_time": "2024-02-06T23:32:47.691937Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvxpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 9\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mempirical_data\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01moptimized_random_response\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrandomized_response\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrepetitive_optimizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[0;32m~/Documents/minimal-data-ldp/optimized_random_response.py:4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mprivacy_mechanism\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutil_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mconvex_optimizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrandomized_response\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Randomized_Response\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mOptimized_Randomized_Response\u001B[39;00m(Privacy_Mechanism):\n",
      "File \u001B[0;32m~/Documents/minimal-data-ldp/convex_optimizer.py:5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m    Convex optimizer using CVXPY.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcvxpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mcp\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutil_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'cvxpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import os\n",
    "import random\n",
    "\n",
    "\n",
    "from empirical_data import *\n",
    "from optimized_random_response import *\n",
    "from randomized_response import *\n",
    "from repetitive_optimizer import *\n",
    "from synthetic_dataset import *\n",
    "from exponential_mechanism import *\n",
    "from simpleinfotheory import *\n",
    "from normalize_error_matrix import *\n",
    "from rappor_mechanism import *\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "mpl.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvxpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcvxpy\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'cvxpy'"
     ]
    }
   ],
   "source": [
    "import cvxpy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T23:35:37.211875Z",
     "start_time": "2024-02-06T23:35:36.925302Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.951517Z"
    }
   },
   "outputs": [],
   "source": [
    "from alphabet import *\n",
    "\n",
    "NUM_ATTRIBUTES = 2\n",
    "NUM_STATES = [3, 3]\n",
    "DATASET_SIZE = 10000\n",
    "\n",
    "priority_dict = {} # {'0': 2, '1': 0.5}\n",
    "\n",
    "TOTAL_STATES = 1\n",
    "alphabet_dict = {}\n",
    "for i in range(NUM_ATTRIBUTES):\n",
    "    TOTAL_STATES *= NUM_STATES[i]\n",
    "    alphabet_dict[str(i)] = np.arange(NUM_STATES[i])\n",
    "\n",
    "ALL_ALPHABET = create_alphabet(attributes_with_alphabet=alphabet_dict)\n",
    "STRING_ALL_ALPHABET = convert_alphabet_to_string(ALL_ALPHABET)\n",
    "ATTRIBUTE_LIST = list(alphabet_dict.keys())\n",
    "# random_dist = np.array([random.randint(1, 5000) for i in range(TOTAL_STATES)])\n",
    "# random_dist = random_dist/np.sum(random_dist)\n",
    "random_dist = [0.3, 0.01, 0.01, 0.01, 0.30, 0.01, 0.01, 0.01, 0.34] # [0.2, 0.11, 0.01, 0.01, 0.20, 0.11, 0.01, 0.11, 0.24] # [0.3, 0.01, 0.01, 0.01, 0.30, 0.01, 0.01, 0.01, 0.34]\n",
    "# random_dist = [0.25, 0.25, 0.25, 0.25] # [0.5, 0.025, 0.025, 0.45]\n",
    "\n",
    "print(\"Alphabet \", ALL_ALPHABET)\n",
    "print(\"ATTRIBUTE_LIST \", ATTRIBUTE_LIST)\n",
    "\n",
    "synthetic_dataset_constructor = Gen_Synthetic_Dataset(no_of_states = TOTAL_STATES, no_of_samples = DATASET_SIZE, alphabet=STRING_ALL_ALPHABET)\n",
    "correlated_synthetic_dataset = synthetic_dataset_constructor.gen_custom(distribution=random_dist)\n",
    "\n",
    "uniform_synthetic_dataset = synthetic_dataset_constructor.gen_uniform()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claculate Error Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.954573Z"
    }
   },
   "outputs": [],
   "source": [
    "# alphabet_dict = {}\n",
    "# for i in range(len(ALL_ALPHABET)):\n",
    "#     alphabet_dict[str(ALL_ALPHABET[i])] = i\n",
    "# print(alphabet_dict)\n",
    "# normalize_error_matrix = Normalize_error_matrix(attribute_list=ATTRIBUTE_LIST, alphabet=ALL_ALPHABET, priority_dict=priority_dict, alphabet_dict=alphabet_dict)\n",
    "# err_matrix = normalize_error_matrix.normalized_error_matrix\n",
    "# sns.heatmap(err_matrix)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair-wise information leakage of A: I(A;A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.957258Z"
    }
   },
   "outputs": [],
   "source": [
    "def mutual_info_pair(a, b):\n",
    "    return mutualinformationempirical(xn=a, yn=b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total information leakage of A: I(A;A',B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.959836Z"
    }
   },
   "outputs": [],
   "source": [
    "# I(A;A',B') = I(A;A') + I(A;B'|A')\n",
    "\n",
    "def conditional_mutual_info(a, b, c):\n",
    "    return conditionalmutualinformationempirical(a, b, c)\n",
    "\n",
    "def total_info_leakage(A, A_prime, B_prime):\n",
    "    return  mutual_info_pair(A, A_prime) + conditional_mutual_info(A, B_prime, A_prime)\n",
    "    # return  mutual_info_pair(A, B_prime) - conditional_mutual_info(A, B_prime, A_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.962946Z"
    }
   },
   "outputs": [],
   "source": [
    "def mutual_information(a, b):\n",
    "    processed_a = []\n",
    "    # print(a)\n",
    "    # print(b)\n",
    "    for i in a:\n",
    "        actual_split = i.split(\" \")\n",
    "        actual = []\n",
    "        # print(actual_split)\n",
    "        for j in actual_split:\n",
    "            # print(i)\n",
    "            if j != \"\" :\n",
    "                actual.append(int(j))\n",
    "        processed_a.append(np.array(actual))\n",
    "    processed_a = np.array(processed_a)\n",
    "    processed_b = []\n",
    "    \n",
    "    for i in b:\n",
    "        actual_split = i.split(\" \")\n",
    "        actual = []\n",
    "        # print(actual_split)\n",
    "        for j in actual_split:\n",
    "            # print(i)\n",
    "            if j != \"\" :\n",
    "                actual.append(int(j))\n",
    "        processed_b.append(np.array(actual))\n",
    "    processed_b = np.array(processed_b)\n",
    "\n",
    "    mi = []\n",
    "    # print(\"processed_a\", processed_a)\n",
    "    # print(\"processed_b\", processed_b)\n",
    "    # for i in range(np.shape(processed_b)[1]):\n",
    "    #     mi.append(mutual_info_pair(processed_a[:,i], processed_b[:,i]))\n",
    "    # mi.append(mutual_info_pair(processed_a[:,0], processed_a[:,1]))\n",
    "    # mi.append(mutual_info_pair(processed_b[:,1], processed_b[:,0]))\n",
    "    # mi.append(mutual_info_pair(processed_a[:,0], processed_b[:,0]))\n",
    "    # mi.append(mutual_info_pair(processed_a[:,1], processed_b[:,1]))\n",
    "    # mi.append(total_info_leakage(processed_a[:,0], processed_b[:,0], processed_b[:,1]))\n",
    "    mi.append(total_info_leakage(processed_a[:,1], processed_b[:,1], processed_b[:,0]))\n",
    "    # mi.append(total_info_leakage(processed_a[:,0], processed_b[:,1], processed_b[:,0]))\n",
    "    return mi\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Optimal Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.966216Z"
    }
   },
   "outputs": [],
   "source": [
    "EPS_ARRAY = np.arange(100)*0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.969046Z"
    }
   },
   "outputs": [],
   "source": [
    "utility_error_function = [\"0_1\", \"l1\", \"l2\"]\n",
    "line_styles = {\"RAPPOR\": \":\", \"k-RR\": \"--\", \"Exponential\": \"-.\", \"Our\": \"-\"}\n",
    "    \n",
    "for util_err in utility_error_function:\n",
    "    alphabet_dict = {}\n",
    "    for i in range(len(ALL_ALPHABET)):\n",
    "        alphabet_dict[str(ALL_ALPHABET[i])] = i\n",
    "    print(alphabet_dict)\n",
    "    normalize_error_matrix = Normalize_error_matrix(attribute_list=ATTRIBUTE_LIST, alphabet=ALL_ALPHABET, priority_dict=priority_dict, alphabet_dict=alphabet_dict, err_type=util_err)\n",
    "    err_matrix = normalize_error_matrix.normalized_error_matrix\n",
    "    sns.heatmap(err_matrix)\n",
    "    plt.show()\n",
    "\n",
    "    exponential_mechanism_without_prior = Exponential_mechanism(prior_dist=random_dist, STATE_COUNT=TOTAL_STATES, INPUT_ALPHABET=STRING_ALL_ALPHABET, normalized_objective_err_matrix=err_matrix, only_err_matrix=True)\n",
    "\n",
    "    exponential_mechanism_with_prior = Exponential_mechanism(prior_dist=random_dist, STATE_COUNT=TOTAL_STATES, INPUT_ALPHABET=STRING_ALL_ALPHABET, normalized_objective_err_matrix=err_matrix, only_err_matrix=False)\n",
    "\n",
    "    random_response_mechanism = Randomized_Response(STATE_COUNT=TOTAL_STATES, INPUT_ALPHABET=STRING_ALL_ALPHABET, normalized_objective_err_matrix=err_matrix)\n",
    "\n",
    "    optimal_random_response_mechanism = Optimized_Randomized_Response(prior_dist = random_dist, STATE_COUNT = TOTAL_STATES, INPUT_ALPHABET = STRING_ALL_ALPHABET, normalized_objective_err_matrix = err_matrix, \n",
    "                    TOLERANCE_MARGIN = 0.01, APPROXIMATION = \"LINEAR\", solver = \"SCS\", is_kl_div = True, ALPHA=0.01, accelerate_from_rr=True)\n",
    "\n",
    "    rappor_mechanism = Rappor_mechanism(STATE_COUNT=TOTAL_STATES, INPUT_ALPHABET = STRING_ALL_ALPHABET, prob_f=0.5, prob_p=0.25, prob_q=0.75, collection_count = 20)\n",
    "    # mechanisms_dict = {\"Exp. mechanism without prior\": exponential_mechanism_without_prior, \"Exp. mechanism with prior\": exponential_mechanism_with_prior, \"k-RR\": random_response_mechanism, \"Optimal k-RR\": optimal_random_response_mechanism}\n",
    "    # mechanisms_dict = {\"k-RR\": random_response_mechanism, \"Optimal k-RR\": optimal_random_response_mechanism}\n",
    "    mechanisms_dict = {\"RAPPOR\": rappor_mechanism, \"k-RR\": random_response_mechanism, \"Exponential\": exponential_mechanism_without_prior, \"Our\": optimal_random_response_mechanism}\n",
    "\n",
    "    error_dict = {}\n",
    "    mi_dict = {}\n",
    "    total_mi_dict = {}\n",
    "\n",
    "    for key in list(mechanisms_dict.keys()):\n",
    "        error_dict[key] = []\n",
    "        mi_dict[key] = []\n",
    "        total_mi_dict[key] = []\n",
    "\n",
    "    for eps in EPS_ARRAY:\n",
    "        # print(eps)\n",
    "        for mechanism in list(mechanisms_dict.keys()):\n",
    "            __tot_error = 0\n",
    "            __perturbed_value_list = []\n",
    "            for entry in correlated_synthetic_dataset:\n",
    "                # entry = entry_np_value[0]\n",
    "                if mechanism == \"RAPPOR\":\n",
    "                    __perturbed_value_list.append(mechanisms_dict[mechanism].gen_random_output(actual_value=entry, eps=eps, prob_f=eps/10, is_eps=True)[0])\n",
    "                else:\n",
    "                    __perturbed_value_list.append(mechanisms_dict[mechanism].gen_random_output(actual_value=entry, eps=eps)[0])\n",
    "                # print(entry,__perturbed_value_list[-1])\n",
    "                __error = normalize_error_matrix.get_value_error(actual=entry, perturbed=__perturbed_value_list[-1])\n",
    "                __tot_error += __error\n",
    "            error_dict[mechanism].append(__tot_error/len(uniform_synthetic_dataset))\n",
    "            mi_dict[mechanism].append(mutual_info_pair(correlated_synthetic_dataset, __perturbed_value_list))\n",
    "            # total_mi_dict[mechanism].append(total_info_leakage(correlated_synthetic_dataset, __perturbed_value_list, ))\n",
    "            # mi_dict[mechanism].append(mutual_information(correlated_synthetic_dataset, __perturbed_value_list))\n",
    "\n",
    "            # if mechanism == \"Optimal k-RR\":\n",
    "            #     optimal_mechanism = mechanisms_dict[mechanism].get_mechanism(eps=eps)\n",
    "                \n",
    "                # posterior_prob_distribution = np.zeros(TOTAL_STATES)\n",
    "                # posterior_joint_prob = np.zeros((TOTAL_STATES, TOTAL_STATES))\n",
    "                # for i in range(TOTAL_STATES):\n",
    "                #     posterior_joint_prob[:,i] = np.reshape(optimal_mechanism[:,i] * np.array(random_dist), (4,))\n",
    "                #     posterior_prob_distribution[i] = np.sum(optimal_mechanism[:,i] * np.array(random_dist))\n",
    "                # print(\"posterior_prob_distribution \", posterior_prob_distribution)\n",
    "                # print(\"Distance \", np.linalg.norm((random_dist-posterior_prob_distribution), 2))\n",
    "                # sns.heatmap(optimal_mechanism)\n",
    "                # plt.show()\n",
    "    lagends = []\n",
    "    for mechanism in list(mechanisms_dict.keys()):\n",
    "        lagends.append(mechanism + \" utility_error\")\n",
    "        plt.plot(EPS_ARRAY, error_dict[mechanism])\n",
    "        # lagends.append(mechanism + \" info_leakage\")\n",
    "        # plt.plot(EPS_ARRAY, mi_dict[mechanism])\n",
    "        # lagends.append(mechanism + \" total_info_leakage\")\n",
    "        # plt.plot(EPS_ARRAY, total_mi_dict[mechanism])\n",
    "        # for mi_ in range(len(mi_dict[mechanism][0])):\n",
    "        plt.plot(EPS_ARRAY, np.array(mi_dict[mechanism]))\n",
    "        #     lagends.append(f\"{mechanism}\")\n",
    "            # legend.append(f\"MI for Optimal, attr:{grouped_attributes_list[i][mi_]}\")\n",
    "\n",
    "    plt.legend(lagends)\n",
    "    plt.xlabel(\"Privacy Budget (Epsilon)\")\n",
    "    plt.ylabel(\"Error (Normalized)\")\n",
    "    # plt.title(f\"{error_cal_type} error between original value and perturbed value for {Attr_list}\")\n",
    "    plt.show()\n",
    "\n",
    "    utility_error = np.arange(26)\n",
    "    \n",
    "    utility_error = utility_error/np.max(utility_error)\n",
    "\n",
    "    tolerance = (utility_error[1] - utility_error[0])/2\n",
    "\n",
    "    info_dict = {}\n",
    "\n",
    "    for mechanism in list(mechanisms_dict.keys()):\n",
    "        info_dict[mechanism] = []\n",
    "        for i in utility_error:\n",
    "            info_dict[mechanism].append(np.nan)\n",
    "\n",
    "    for index_1, i in enumerate(utility_error):\n",
    "        for mechanism in list(mechanisms_dict.keys()):\n",
    "            for index_, j in enumerate(error_dict[mechanism]):\n",
    "                if abs(i-j) < tolerance:\n",
    "                    info_dict[mechanism][index_1] = (mi_dict[mechanism][index_])\n",
    "                    break\n",
    "    fig = plt.figure()\n",
    "    lagends = []\n",
    "    for mechanism in list(mechanisms_dict.keys()):\n",
    "        lagends.append(mechanism)\n",
    "        plt.plot(utility_error, info_dict[mechanism], linestyle=line_styles[mechanism])\n",
    "\n",
    "    plt.legend(lagends)\n",
    "    plt.grid(visible=True)\n",
    "    plt.show()\n",
    "    fig.savefig(f\"{util_err}.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.971739Z"
    }
   },
   "outputs": [],
   "source": [
    "utility_error = np.arange(30)\n",
    "    \n",
    "utility_error = utility_error/np.max(utility_error)\n",
    "\n",
    "tolerance = (utility_error[1] - utility_error[0])/2\n",
    "\n",
    "info_dict = {}\n",
    "\n",
    "for mechanism in list(mechanisms_dict.keys()):\n",
    "    info_dict[mechanism] = []\n",
    "    for i in utility_error:\n",
    "        info_dict[mechanism].append(np.nan)\n",
    "\n",
    "for index_1, i in enumerate(utility_error):\n",
    "    for mechanism in list(mechanisms_dict.keys()):\n",
    "        for index_, j in enumerate(error_dict[mechanism]):\n",
    "            if abs(i-j) < tolerance:\n",
    "                info_dict[mechanism][index_1] = (mi_dict[mechanism][index_])\n",
    "                break\n",
    "fig = plt.figure()\n",
    "lagends = []\n",
    "for mechanism in list(mechanisms_dict.keys()):\n",
    "    lagends.append(mechanism)\n",
    "    plt.plot(utility_error, info_dict[mechanism], linestyle=line_styles[mechanism])\n",
    "\n",
    "plt.legend(lagends)\n",
    "plt.grid(visible=True)\n",
    "plt.show()\n",
    "fig.savefig(f\"{util_err}.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T22:15:35.036789Z",
     "start_time": "2024-02-06T22:15:34.973172Z"
    }
   },
   "outputs": [],
   "source": [
    "for mechanism in list(mechanisms_dict.keys()):\n",
    "    lagends.append(mechanism + \" utility_error\")\n",
    "    plt.plot(EPS_ARRAY, error_dict[mechanism])\n",
    "    # lagends.append(mechanism + \" info_leakage\")\n",
    "    # plt.plot(EPS_ARRAY, mi_dict[mechanism])\n",
    "    # lagends.append(mechanism + \" total_info_leakage\")\n",
    "    # plt.plot(EPS_ARRAY, total_mi_dict[mechanism])\n",
    "    plt.plot(EPS_ARRAY, np.array(mi_dict[mechanism]))\n",
    "    #     lagends.append(f\"{mechanism}\")\n",
    "        # legend.append(f\"MI for Optimal, attr:{grouped_attributes_list[i][mi_]}\")\n",
    "\n",
    "plt.legend(lagends)\n",
    "plt.xlabel(\"Privacy Budget (Epsilon)\")\n",
    "plt.ylabel(\"Error (Normalized)\")\n",
    "# plt.title(f\"{error_cal_type} error between original value and perturbed value for {Attr_list}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.974606Z"
    }
   },
   "outputs": [],
   "source": [
    "error_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.976020Z"
    }
   },
   "outputs": [],
   "source": [
    "utility_error = np.arange(20)\n",
    "utility_error = utility_error/np.max(utility_error)\n",
    "\n",
    "tolerance = (utility_error[1] - utility_error[0])/2\n",
    "\n",
    "info_dict = {}\n",
    "\n",
    "for mechanism in list(mechanisms_dict.keys()):\n",
    "    info_dict[mechanism] = []\n",
    "    for i in utility_error:\n",
    "        info_dict[mechanism].append(np.nan)\n",
    "\n",
    "for index_1, i in enumerate(utility_error):\n",
    "    for mechanism in list(mechanisms_dict.keys()):\n",
    "        for index_, j in enumerate(error_dict[mechanism]):\n",
    "            if abs(i-j) < tolerance:\n",
    "                info_dict[mechanism][index_1] = (mi_dict[mechanism][index_])\n",
    "                break\n",
    "\n",
    "lagends = []\n",
    "for mechanism in list(mechanisms_dict.keys()):\n",
    "    lagends.append(mechanism + \" utility_error\")\n",
    "    plt.plot(utility_error, info_dict[mechanism])\n",
    "\n",
    "plt.legend(lagends)\n",
    "plt.grid(visible=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.977458Z"
    }
   },
   "outputs": [],
   "source": [
    "info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.978964Z"
    }
   },
   "outputs": [],
   "source": [
    "for mechanism in list(mechanisms_dict.keys()):\n",
    "    lagends.append(mechanism + \" utility_error\")\n",
    "    plt.plot(utility_error, info_dict[mechanism])\n",
    "plt.plot(utility_error, aa)\n",
    "plt.legend(lagends)\n",
    "plt.grid(visible=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.980582Z"
    }
   },
   "outputs": [],
   "source": [
    "# aa = info_dict[\"Optimal k-RR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-06T22:15:34.982148Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data with some points set to np.nan\n",
    "x = np.array([0, 1, 2, 3, 4, 5])\n",
    "y = np.array([2, 3, np.nan, 5, np.nan, 6])\n",
    "\n",
    "# Plotting\n",
    "plt.plot(x, y)\n",
    "plt.title('Plot with Skipped Points')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
